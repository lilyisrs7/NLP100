{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'word_id_dict.json'\n",
    "with open(filename) as f:\n",
    "    word_id = load(f)\n",
    "vocab_size = max(word_id.values())+2 # unk, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, embedding_dim=300, hidden_size=50, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = torch.argmax(input, dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        o, x = self.rnn(x)\n",
    "        # print(x.shape)\n",
    "        x = self.linear(x[0])\n",
    "        # print(x.shape)\n",
    "        out = self.softmax(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10672 1334 1334\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.feature.txt')\n",
    "df_valid = pd.read_csv('valid.feature.txt')\n",
    "df_test = pd.read_csv('test.feature.txt')\n",
    "\n",
    "def tokenize(input):\n",
    "    return [word_id[word] if word_id.get(word) is not None else 0 for word in input.split()]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input\": nn.functional.one_hot(torch.tensor(tokenize(self.df[\"title\"][idx]), dtype=torch.int64), num_classes=vocab_size),\n",
    "            \"label\": torch.tensor(self.df[\"category\"][idx], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "train_data = CustomDataset(df_train[[\"title\", \"category\"]])\n",
    "valid_data = CustomDataset(df_valid[[\"title\", \"category\"]])\n",
    "test_data = CustomDataset(df_test[[\"title\", \"category\"]])\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data)\n",
    "test_dataloader = DataLoader(test_data)\n",
    "print(len(train_dataloader), len(valid_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(task='multiclass', num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "train loss: 1.3070982587446962 train acc: 0.42016491293907166\n",
      "valid loss: 1.2693372653878254 valid acc: 0.4947526156902313\n",
      "epochs: 1\n",
      "train loss: 1.238513960166537 train acc: 0.524550199508667\n",
      "valid loss: 1.2267499137139213 valid acc: 0.5269864797592163\n",
      "epochs: 2\n",
      "train loss: 1.2011945720488342 train acc: 0.5493815541267395\n",
      "valid loss: 1.204099293323471 valid acc: 0.5344827771186829\n",
      "epochs: 3\n",
      "train loss: 1.1750376168442989 train acc: 0.5715892314910889\n",
      "valid loss: 1.1853234325123692 valid acc: 0.552473783493042\n",
      "epochs: 4\n",
      "train loss: 1.1518567335335301 train acc: 0.595108687877655\n",
      "valid loss: 1.1730596088487348 valid acc: 0.5622189044952393\n",
      "epochs: 5\n",
      "train loss: 1.1289590145370503 train acc: 0.6212518811225891\n",
      "valid loss: 1.1451826036601946 valid acc: 0.6019490361213684\n",
      "epochs: 6\n",
      "train loss: 1.1044113980843566 train acc: 0.6447713375091553\n",
      "valid loss: 1.1197424205257438 valid acc: 0.6259370446205139\n",
      "epochs: 7\n",
      "train loss: 1.078777679170983 train acc: 0.668571949005127\n",
      "valid loss: 1.1082589878760714 valid acc: 0.6364318132400513\n",
      "epochs: 8\n",
      "train loss: 1.053936629476531 train acc: 0.6928410530090332\n",
      "valid loss: 1.0696635621151644 valid acc: 0.6799100637435913\n",
      "epochs: 9\n",
      "train loss: 1.0353492769947474 train acc: 0.7106446623802185\n",
      "valid loss: 1.0549223443110665 valid acc: 0.6926536560058594\n",
      "test acc: 0.691154420375824\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(\"epochs:\", i)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for data in train_dataloader:\n",
    "        x, y = data[\"input\"], data[\"label\"]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "        labels.append(y.numpy())    \n",
    "    \n",
    "    preds = torch.tensor(np.concatenate(preds))\n",
    "    labels = torch.tensor(np.concatenate(labels))\n",
    "    print('train loss:', total_train_loss / len(train_dataloader), 'train acc:', float(accuracy(preds, labels)))\n",
    "    \n",
    "    # valid\n",
    "    model.eval()\n",
    "    total_valid_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for data in valid_dataloader:\n",
    "        x, y = data[\"input\"], data[\"label\"]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        total_valid_loss += loss.item()\n",
    "        \n",
    "        preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "        labels.append(y.numpy())\n",
    "    \n",
    "    preds = torch.tensor(np.concatenate(preds))\n",
    "    labels = torch.tensor(np.concatenate(labels))\n",
    "    print('valid loss:', total_valid_loss / len(valid_dataloader), 'valid acc:', float(accuracy(preds, labels)))\n",
    "    \n",
    "# test\n",
    "preds = []\n",
    "labels = []\n",
    "for data in test_dataloader:\n",
    "    x, y = data[\"input\"], data[\"label\"]\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "    labels.append(y.numpy())\n",
    "\n",
    "preds = torch.tensor(np.concatenate(preds))\n",
    "labels = torch.tensor(np.concatenate(labels))\n",
    "print('test acc:', float(accuracy(preds, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
