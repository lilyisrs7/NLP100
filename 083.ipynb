{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'word_id_dict.json'\n",
    "with open(filename) as f:\n",
    "    word_id = load(f)\n",
    "vocab_size = max(word_id.values())+2 # unk, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, embedding_dim=300, hidden_size=50, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = torch.argmax(input, dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        o, x = self.rnn(x)\n",
    "        # print(x.shape)\n",
    "        x = self.linear(x[0])\n",
    "        # print(x.shape)\n",
    "        out = self.softmax(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2668 1334 1334\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.feature.txt')\n",
    "df_valid = pd.read_csv('valid.feature.txt')\n",
    "df_test = pd.read_csv('test.feature.txt')\n",
    "\n",
    "def tokenize(input):\n",
    "    return [word_id[word] if word_id.get(word) is not None else 0 for word in input.split()]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input\": nn.functional.one_hot(torch.tensor(tokenize(self.df[\"title\"][idx]), dtype=torch.int64), num_classes=vocab_size),\n",
    "            \"label\": torch.tensor(self.df[\"category\"][idx], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    max_len = 0\n",
    "    for data in batch:\n",
    "        inputs.append(data['input'])\n",
    "        labels.append(data['label'])\n",
    "        max_len = max(max_len, len(data['input']))\n",
    "    pad = [0]*(vocab_size-1)+[1]\n",
    "    return {\n",
    "        'input': torch.stack([torch.cat([input, torch.tensor([pad]*(max_len-len(input)), dtype=torch.int64)]) for input in inputs]),\n",
    "        'label': torch.tensor(labels, dtype=torch.int64)\n",
    "    }\n",
    "\n",
    "train_data = CustomDataset(df_train[[\"title\", \"category\"]])\n",
    "valid_data = CustomDataset(df_valid[[\"title\", \"category\"]])\n",
    "test_data = CustomDataset(df_test[[\"title\", \"category\"]])\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data)\n",
    "test_dataloader = DataLoader(test_data)\n",
    "print(len(train_dataloader), len(valid_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(task='multiclass', num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "train loss: 1.2552933269801705 train acc: 0.4860382378101349\n",
      "valid loss: 1.250510450245916 valid acc: 0.4790104925632477\n",
      "epochs: 1\n",
      "train loss: 1.1678686853738383 train acc: 0.5723388195037842\n",
      "valid loss: 1.1355501987944836 valid acc: 0.617691159248352\n",
      "epochs: 2\n",
      "train loss: 1.1086329900432026 train acc: 0.6346514225006104\n",
      "valid loss: 1.1301916982905975 valid acc: 0.6101949214935303\n",
      "epochs: 3\n",
      "train loss: 1.143845460873315 train acc: 0.6001686453819275\n",
      "valid loss: 1.1949274832162184 valid acc: 0.5494752526283264\n",
      "epochs: 4\n",
      "train loss: 1.1587060728798741 train acc: 0.5833020806312561\n",
      "valid loss: 1.149800824663271 valid acc: 0.5854572653770447\n",
      "epochs: 5\n",
      "train loss: 1.1113551015528602 train acc: 0.6308096051216125\n",
      "valid loss: 1.1215149699926734 valid acc: 0.6146926283836365\n",
      "epochs: 6\n",
      "train loss: 1.0899765434070328 train acc: 0.6530172228813171\n",
      "valid loss: 1.0956461947897207 valid acc: 0.6454272866249084\n",
      "epochs: 7\n",
      "train loss: 1.0778511733576275 train acc: 0.6654797792434692\n",
      "valid loss: 1.1163157435103335 valid acc: 0.6229385137557983\n",
      "epochs: 8\n",
      "train loss: 1.1068620812678445 train acc: 0.6363380551338196\n",
      "valid loss: 1.2326247286850187 valid acc: 0.5104947686195374\n",
      "epochs: 9\n",
      "train loss: 1.2214095855000495 train acc: 0.5212706327438354\n",
      "valid loss: 1.2175390388833351 valid acc: 0.5247376561164856\n",
      "test acc: 0.5427286624908447\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(\"epochs:\", i)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for data in train_dataloader:\n",
    "        x, y = data[\"input\"], data[\"label\"]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "        labels.append(y.numpy())    \n",
    "    \n",
    "    preds = torch.tensor(np.concatenate(preds))\n",
    "    labels = torch.tensor(np.concatenate(labels))\n",
    "    print('train loss:', total_train_loss / len(train_dataloader), 'train acc:', float(accuracy(preds, labels)))\n",
    "    \n",
    "    # valid\n",
    "    model.eval()\n",
    "    total_valid_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for data in valid_dataloader:\n",
    "        x, y = data[\"input\"], data[\"label\"]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        total_valid_loss += loss.item()\n",
    "        \n",
    "        preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "        labels.append(y.numpy())\n",
    "    \n",
    "    preds = torch.tensor(np.concatenate(preds))\n",
    "    labels = torch.tensor(np.concatenate(labels))\n",
    "    print('valid loss:', total_valid_loss / len(valid_dataloader), 'valid acc:', float(accuracy(preds, labels)))\n",
    "    \n",
    "# test\n",
    "preds = []\n",
    "labels = []\n",
    "for data in test_dataloader:\n",
    "    x, y = data[\"input\"], data[\"label\"]\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    preds.append(torch.argmax(y_pred, dim=1).numpy())\n",
    "    labels.append(y.numpy())\n",
    "\n",
    "preds = torch.tensor(np.concatenate(preds))\n",
    "labels = torch.tensor(np.concatenate(labels))\n",
    "print('test acc:', float(accuracy(preds, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
